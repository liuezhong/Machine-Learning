
                             一、机器学习的定义

1、非正式定义
    1959年Arthur Samuel将机器学习非正式地定义为：在不直接针对问题进行编程的情况下，赋予计算机学习能力的研究领域。（Arthur Samuel曾经写过一个西洋棋程序，可以自己和自己下棋。Arthur Samuel让他自己的程序自己和自己下了成千上万盘棋，逐渐地，程序慢慢的意识到，怎样的局势能导致胜利，怎样的局势能导致失败。它经过反复的学习，“如果让对手的棋子占据了这些地方，那么我输的概率会比较大。或者我的棋子占据了这些地方，那么我赢得概率会比较大”。1959年，出现了一个奇迹，Arthur Samuel的程序的棋艺甚至远远的超过了他自己。Arthur Samuel的西洋棋程序是第一个对于“计算机除了做程序明确让它做的事情外不能做任何事情”这种观点的有力反驳。Arthur Samuel并没有明确告诉计算机如何下棋，而是让计算机自己学习。
2、更现代化的定义
    1998年Tom Mitchell提出一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。（对于西洋棋程序的例子来说，经验E对应着程序不断和自己下棋的经历，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率）

                              二、监督学习（Supervised Learning)
之所以被称为监督学习，是因为这个算法是“监督”问题的算法，换句话说，我们给算法提供了一组“标准答案”，之后我们希望算法去学习标准输入和标准答案之间的关系以尝试对于我们的其他输入，给我们提供更为标准的答案。
监督学习有两类问题：“回归”和“分类”问题
“回归”问题（Regression）：我们需要预测的变量是连续的。
“分类”问题（Classification）：我们需要预测的变量是离散的。支持向量机的算法可以把数据映射到无限维空间中，即可以处理无限多种特性，即处理无限维的变量
Part 1：
（一）线性回归（Linear Regression)
θ被称为学习算法的参数，都是实数，利用训练集合选择或学习得到合适的参数值是学习算法的任务。
下面介绍几种算法来选取θ使得J(θ)最小：
（1）梯度下降（Gradient Descent)：从一个初始点沿着梯度下降最快的方向前进。
    梯度下降算法会重复向最陡的方向下降的步骤，
    我们可以把这个步骤写成这样的形式：
    a:= b是表示左边变量的值设成右边变量的值，表示这是计算机程序的一部分或者说是算法的一部分。mine:赋值。
    a=b是一个真值断言，想表达的是a的值与b的值相等。
    梯度下降算法中，α是一个算法参数，称为学习速度，它控制着你迈的步子有多大（你站在山上，往梯度下降最快的方向走）
    有几种不同的方式检测收敛：
    （a）检验两次迭代，看两次迭代中是否改变了很多，如果在两次迭代中没怎么改变，则可以说算法有可能收敛了。
    （b）更常用的方法是，你检验的值或许是你试图最小化的量不再发生很大的改变。

     批梯度下降算法（Batch Gradient Descent)：每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练集合样本进行求和。mine:更新一个参数需用到所有训练集
     随机梯度下降算法或增量梯度下降算法(Stochastic Gradient Descent或者incremental Gradient Descent)：每一次迭代内部会用类似于梯度下降的方式更新参数的值，最后算法结束的时候仅仅会使用i个样本。它的好处是:为了开始学习，为了开始修改参数，你仅仅需要查看第一个训练样本，你应该查看你的第一个训练样本并且利用第一个进行更新，之后你需要使用第二个训练样本执行下一次更新，这样你调整参数的速度会快的多，因为你不需要在调整之前遍历所有的样本数据。虽然随机梯度下降算法通常会快的多，但是随机梯度下降算法不会精确地收敛到全局的最小值，参数总体趋向于想着全局最小值附近徘徊，可能会一直徘徊，通常得到的参数值能够很接近全局最小值。mine：更新一个参数只需用到一个训练样本。
    如果你有一个很大的训练集合（大规模的数据集）的时候，那么你应该使用随机梯度下降算法

  (2)  矩阵求导

（二）局部加权回归（Locally weighted regression）线性回归的一个变化版本
    欠拟合（underfitting）：非正式的理解为一种情形，在这种情形下，数据中的某些非常明显的模式没有被成功的拟合出来。
    过拟合（overfitting）：非正式的理解为一种情形，在这种情形下，算法拟合出的结果仅仅反映了所给的特定数据的特质。例如将7个特征数据拟合出的6次多项式函数仅仅反映了该数据集合的特质，而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。
    有几种方法可以解决欠拟合及过拟合这类问题。
    （a）特征选择算法，这是一类自动化的算法，可以在这类回归问题中选择要用到的特征。
    （b）非参数学习算法，可以缓解对于选取特征的需求。
    参数学习算法（parametric learning algorithm）：是一类有固定数目的参数以用来进行数据拟合的算法。线性回归是参数学习算法的一个例子。
    非参数学习算法（non-parametric learning algorithm)：是一个参数数量会随着m（训练集合的大小）增长的算法。

    局部加权回归（Locally weighted regression）又名（loess/lowess)：
    对于你想知道x在某一位置处的y的预测值？
    线性回归的做法：首先拟合出θ使得J(θ)最小，之后返回h(x)。
    局部加权回归的做法：首先检查数据集合并且只考虑那些位于x周围固定区域内的数据点，之后对这个数据子集使用线性回归来拟合出一条直线，根据这条直线求出具体的值作为我的算法的返回结果，这将是局部加权回归中的假设的输出结果（预测值）。
    Ƭ叫做波长函数，它控制着权值随距离下降的速度，如果Ƭ越小，就会得到一个相当窄的“高斯函数”。
Part 2
（二）概率解释（Probabilistic interpretation）对于线性回归的解释
（三）logistic回归（Logistic regression）一种分类算法
（四）感知器（Digression perceptron）
（五）牛顿方法（Newton's method）一个用来对logistic回归模型进行拟合的算法


                              三、学习理论（Learning Theory)
 通过学习理论，让我们理解为什么学习型算法是有效的，这样我们才可以尽可能高效的的工作。我们将对学习理论进行一些深入研究以试图了解什么样的算法能很好地近视不同的函数，并且试图了解一些诸如需要多少训练数据这样的问题。
                              四、无监督学习（Unsupervised Learning)
 无监督学习：给你一组数据，然后不告诉你关于数据的任何正确答案，给你数据后会这样问你“你能在这组数据中寻找一些有趣的结构吗？”
 聚类是无监督学习的例子
                              五、强化学习（Reinforcement Learning)
强化学习：它可以被用在你不需要进行一次决策的情形中，在强化学习的过程中，你通常会在一段时间内做出一系列的决策。例如：用电脑控制飞机飞起来，你只要连续地做出一系列总体上还不错的决策，就可以让飞机飞起来。
基本概念是称为回报函数的概念