
                             一、机器学习的定义

1、非正式定义
    1959年Arthur Samuel将机器学习非正式地定义为：在不直接针对问题进行编程的情况下，赋予计算机学习能力的研究领域。（Arthur Samuel曾经写过一个西洋棋程序，可以自己和自己下棋。Arthur Samuel让他自己的程序自己和自己下了成千上万盘棋，逐渐地，程序慢慢的意识到，怎样的局势能导致胜利，怎样的局势能导致失败。它经过反复的学习，“如果让对手的棋子占据了这些地方，那么我输的概率会比较大。或者我的棋子占据了这些地方，那么我赢得概率会比较大”。1959年，出现了一个奇迹，Arthur Samuel的程序的棋艺甚至远远的超过了他自己。Arthur Samuel的西洋棋程序是第一个对于“计算机除了做程序明确让它做的事情外不能做任何事情”这种观点的有力反驳。Arthur Samuel并没有明确告诉计算机如何下棋，而是让计算机自己学习。
2、更现代化的定义
    1998年Tom Mitchell提出一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。（对于西洋棋程序的例子来说，经验E对应着程序不断和自己下棋的经历，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率）

                              二、监督学习（Supervised Learning)
之所以被称为监督学习，是因为这个算法是“监督”问题的算法，换句话说，我们给算法提供了一组“标准答案”，之后我们希望算法去学习标准输入和标准答案之间的关系以尝试对于我们的其他输入，给我们提供更为标准的答案。
监督学习有两类问题：“回归”和“分类”问题
“回归”问题（Regression）：我们需要预测的变量是连续的。
“分类”问题（Classification）：我们需要预测的变量是离散的。支持向量机的算法可以把数据映射到无限维空间中，即可以处理无限多种特性，即处理无限维的变量
Part 1：
（一）线性回归（Linear Regression)
θ被称为学习算法的参数，都是实数，利用训练集合选择或学习得到合适的参数值是学习算法的任务。
下面介绍几种算法来选取θ使得J(θ)最小：
（1）梯度下降（Gradient Descent)：从一个初始点沿着梯度下降最快的方向前进。
    梯度下降算法会重复向最陡的方向下降的步骤，
    我们可以把这个步骤写成这样的形式：
    a:= b是表示左边变量的值设成右边变量的值，表示这是计算机程序的一部分或者说是算法的一部分。mine:赋值。
    a=b是一个真值断言，想表达的是a的值与b的值相等。
    梯度下降算法中，α是一个算法参数，称为学习速度，它控制着你迈的步子有多大（你站在山上，往梯度下降最快的方向走）
    有几种不同的方式检测收敛：
    （a）检验两次迭代，看两次迭代中是否改变了很多，如果在两次迭代中没怎么改变，则可以说算法有可能收敛了。
    （b）更常用的方法是，检验的值或试图最小化的量不再发生很大的改变。

     批梯度下降算法（Batch Gradient Descent)：每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练集合样本进行求和。mine:更新一个参数需用到所有训练集
     随机梯度下降算法或增量梯度下降算法(Stochastic Gradient Descent或者incremental Gradient Descent)：每一次迭代内部会用类似于梯度下降的方式更新参数的值，最后算法结束的时候仅仅会使用i个样本。它的好处是:为了开始学习，为了开始修改参数，你仅仅需要查看第一个训练样本，你应该查看你的第一个训练样本并且利用第一个进行更新，之后你需要使用第二个训练样本执行下一次更新，这样你调整参数的速度会快的多，因为你不需要在调整之前遍历所有的样本数据。虽然随机梯度下降算法通常会快的多，但是随机梯度下降算法不会精确地收敛到全局的最小值，参数总体趋向于想着全局最小值附近徘徊，可能会一直徘徊，通常得到的参数值能够很接近全局最小值。mine：更新一个参数只需用到一个训练样本。
    如果你有一个很大的训练集合（大规模的数据集）的时候，那么你应该使用随机梯度下降算法

  (2) 正规方程组（The normal equations)

（3）局部加权回归（Locally weighted regression）线性回归的一个变化版本
    欠拟合（underfitting）：非正式的理解为一种情形，在这种情形下，数据中的某些非常明显的模式没有被成功的拟合出来。
    过拟合（overfitting）：非正式的理解为一种情形，在这种情形下，算法拟合出的结果仅仅反映了所给的特定数据的特质。例如将7个特征数据拟合出的6次多项式函数仅仅反映了该数据集合的特质，而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。
    有几种方法可以解决欠拟合及过拟合这类问题。
    （a）特征选择算法，这是一类自动化的算法，可以在这类回归问题中选择要用到的特征。
    （b）非参数学习算法，可以缓解对于选取特征的需求。
    参数学习算法（parametric learning algorithm）：是一类有固定数目的参数以用来进行数据拟合的算法。线性回归是参数学习算法的一个例子。
    非参数学习算法（non-parametric learning algorithm)：是一个参数数量会随着m（训练集合的大小）增长的算法。

    局部加权回归（Locally weighted regression）又名（loess/lowess)：
    对于你想知道x在某一位置处的y的预测值？
    线性回归的做法：首先拟合出θ使得J(θ)最小，之后返回h(x)。
    局部加权回归的做法：首先检查数据集合并且只考虑那些位于x周围固定区域内的数据点，之后对这个数据子集使用线性回归来拟合出一条直线，根据这条直线求出具体的值作为我的算法的返回结果，这将是局部加权回归中的假设的输出结果（预测值）。
    Ƭ叫做波长函数，它控制着权值随距离下降的速度，如果Ƭ越小，就会得到一个相当窄的“高斯函数”。
 （4）概率解释（Probabilistic interpretation）对于线性回归的解释
 给出一组假设来“证实”为什么我们要选择这样的指标，实际上有许多假设足够可以证实为什么我们要选择最小二乘法，这仅仅是其中之一，因为我只提供一组假设，在这组假设下最小二乘法变得有意义，但并不是唯一的一组假设，所以即使我这里描述的一组假设不成立，最小二乘法实际上在很多条件下仍有意义，这有点类似于帮助你并且给出使用最小二乘法的理由。
   ε是误差项，是对未建模的效应的捕获，它表示了一种我们没有捕获到的特征或者也可以看成一种随机噪声。
   极大似然估计，你需要选择参数θ使得数据出现的可能性尽可能的大，所以要选择θ使似然性最大化。或换句话说是选择参数使得数据数据出现的可能性尽可能的大。

Part 2 Classification and Logistic regression分类算法

（5）logistic回归（Logistic regression）一种分类算法
    logistic回归算法中的θ得更新公式和最小二乘回归中进行梯度下降时的公式一模一样，但是它们是完全不同的，因为logistic回归和线性回归定义不同，它已经不再是线性函数了，它是关于θ的logistic函数，所以即使它们表面上相似，相对于基于最小二乘回归退出的下降规则这实际上是一个完全不同的算法。
    实际上会以同样的学习规则结尾并不是巧合，我们之后会更多的讨论通用的学习模型，但是这是一种我们之后将会看到的最为优雅的通用的学习模型，即使我们使用不同的模型，你实际都会以一种看起来相同的学习算法结束
（6）感知器（The perceptron learning algorithm）

下面介绍另一种模型拟合的方法：牛顿算法（Newton's method）
（7）牛顿方法（Newton's method）：可以用来对logistic回归模型进行拟合，通常情况下运行速度会比梯度上升算法快很多。
和梯度上升算法比起来牛顿方法的特点：
优点：算法收敛所需要的迭代次数要少的多。
缺点：每一次迭代，都需要重新计算一次Hessian矩阵的逆，Hessian矩阵是一个n*n的矩阵（或者是n+1*n+1的矩阵，n是特征的数量），如果你要处理的问题中有大量的特征，那么Hessian矩阵的逆的运算将会花费很大的代价，但是对于规模较小，特征数量合理的问题，牛顿方法通常情况下会是一个很快的算法。

Part 3 广义线性模型（Generalized Linear Model)
线性回归算法和logistic回归算法是一类更广泛的算法的特例，这类算法被称为广义线性模型。
(8)指数分布族（The exponential family)
    选定一种函数形式，对于a(η)、b(y)、T(y),我们固定这三个函数那么这个公式就定义了一个概率分布的集合，定义了一类概率分布，这个概率分布以η为参数。

    伯努利分布和高斯分布（正态分布）都是一类分布的特例，这类分布被称为指数分布族。
其实本科中学习过的大部分分布都可以写成指数分布族的形式，正态分布、多元正态分布、伯努利分布（用于对0 1问题建模）、多项式分布（用于对有K个结果的事件建模）、泊松分布（通常用来对计数的过程进行建模，例如一个样本中放射性衰变的数目）、伽马分布、指数分布（伽马分布和指数分布考虑的是正数的分布，所以它们经常被用来对间隔进行建模）、β分布、Dirichlet分布（β分布和Dirichlet分布用来对小数进行建模，是对概率分布进行建模的）、Wishart分布（是协方差矩阵的分布）都可以写成指数分布族的形式。

(9)广义线性模型（Generalized Linear Model)——这是一组非常漂亮的概念将logistic回归和传统的最小二乘模型联系在一起。
推导出指数分布族的广义线性模型

之所以做这三个假设，是因为它会帮我们导出广义线性模型并且得到非常漂亮的算法用来拟合模型，例如泊松回归模型或者对gama分布或指数分布的输出结果进行回归。

softmax回归被普遍认为是logistic回归的推广，logistic回归只处理两种类别，softmax回归可以处理K类而不仅仅是两类。

如果你有一个机器学习问题，y属于K中的一类，你想用softmax回归来解决它，通常情况下，使用这些推导得出最后的预测，那么现在的问题就是怎样拟合出这些参数。而对于给定的任何x和θ应该用多项式分布对y进行建模，所以选择多项式分布最为指数分布族，求得最大似然估计，并且应用梯度上升或牛顿方法之类的方法使似然性最大化。

part 4 Generative Learning algorithms生成学习算法
    我们之前讨论的那些模型实质上都属于判别学习算法，
    判别学习算法：直接学习p(y|x)或学习得到一个假设直接输出0或1
    生成学习算法：用来对p(x|y)（给定所属的类的情况下，显示某种特定特征的概率，即在给定了样本所属类的情况下一个生成模型对样本特征建立概率模型）和p(y)进行建模，



                              三、学习理论（Learning Theory)
 通过学习理论，让我们理解为什么学习型算法是有效的，这样我们才可以尽可能高效的的工作。我们将对学习理论进行一些深入研究以试图了解什么样的算法能很好地近视不同的函数，并且试图了解一些诸如需要多少训练数据这样的问题。
                              四、无监督学习（Unsupervised Learning)
 无监督学习：给你一组数据，然后不告诉你关于数据的任何正确答案，给你数据后会这样问你“你能在这组数据中寻找一些有趣的结构吗？”
 聚类是无监督学习的例子
                              五、强化学习（Reinforcement Learning)
强化学习：它可以被用在你不需要进行一次决策的情形中，在强化学习的过程中，你通常会在一段时间内做出一系列的决策。例如：用电脑控制飞机飞起来，你只要连续地做出一系列总体上还不错的决策，就可以让飞机飞起来。
基本概念是称为回报函数的概念