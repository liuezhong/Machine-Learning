
                             一、机器学习的定义

1、非正式定义
    1959年Arthur Samuel将机器学习非正式地定义为：在不直接针对问题进行编程的情况下，赋予计算机学习能力的研究领域。（Arthur Samuel曾经写过一个西洋棋程序，可以自己和自己下棋。Arthur Samuel让他自己的程序自己和自己下了成千上万盘棋，逐渐地，程序慢慢的意识到，怎样的局势能导致胜利，怎样的局势能导致失败。它经过反复的学习，“如果让对手的棋子占据了这些地方，那么我输的概率会比较大。或者我的棋子占据了这些地方，那么我赢得概率会比较大”。1959年，出现了一个奇迹，Arthur Samuel的程序的棋艺甚至远远的超过了他自己。Arthur Samuel的西洋棋程序是第一个对于“计算机除了做程序明确让它做的事情外不能做任何事情”这种观点的有力反驳。Arthur Samuel并没有明确告诉计算机如何下棋，而是让计算机自己学习。
2、更现代化的定义
    1998年Tom Mitchell提出一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。（对于西洋棋程序的例子来说，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率）

                              二、监督学习（Supervised Learning)

梯度下降（Gradient Descent)：从一个初始点沿着梯度下降最快的方向前进。
梯度下降算法会重复向最陡的方向下降的步骤，
我们可以把这个步骤写成这样的形式：
a:= b是表示左边变量的值设成右边变量的值，表示这是计算机程序的一部分或者说是算法的一部分。mine:赋值。
a=b是一个真值断言，想表达的是a的值与b的值相等。
梯度下降算法中，α是一个算法参数，称为学习速度，它控制着你迈的步子有多大（你站在山上，往梯度下降最快的方向走）
有几种不同的方式检测收敛：
（1）检验两次迭代，看两次迭代中是否改变了很多，如果在两次迭代中没怎么改变，则可以说算法有可能收敛了。
（2）更常用的方法是，你检验的值或许是你试图最小化的量不再发生很大的改变。

     批梯度下降算法（Batch Gradient Descent)：每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练集合样本进行求和。mine:更新一个参数需用到所有训练集
     随机梯度下降算法或增量梯度下降算法(Stochastic Gradient Descent或者incremental Gradient Descent)：每一次迭代内部会用类似于梯度下降的方式更新参数的值，最后算法结束的时候仅仅会使用i个样本。它的好处是:为了开始学习，为了开始修改参数，你仅仅需要查看第一个训练样本，你应该查看你的第一个训练样本并且利用第一个进行更新，之后你需要使用第二个训练样本执行下一次更新，这样你调整参数的速度会快的多，因为你不需要在调整之前遍历所有的样本数据。虽然随机梯度下降算法通常会快的多，但是随机梯度下降算法不会精确地收敛到全局的最小值，参数总体趋向于想着全局最小值附近徘徊，可能会一直徘徊，通常得到的参数值能够很接近全局最小值。mine：更新一个参数只需用到一个训练样本。
    如果你有一个很大的训练集合（大规模的数据集）的时候，那么你应该使用随机梯度下降算法

    矩阵求导


                              三、学习理论（Learning Theory)
                              四、无监督学习（Unsupervised Learning)
 聚类是无监督学习的例子
                              五、强化学习（Reinforcement Learning)
基本概念是称为回报函数的概念