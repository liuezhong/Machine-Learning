
                             一、机器学习的定义

1、非正式定义
    1959年Arthur Samuel将机器学习非正式地定义为：在不直接针对问题进行编程的情况下，赋予计算机学习能力的研究领域。（Arthur Samuel曾经写过一个西洋棋程序，可以自己和自己下棋。Arthur Samuel让他自己的程序自己和自己下了成千上万盘棋，逐渐地，程序慢慢的意识到，怎样的局势能导致胜利，怎样的局势能导致失败。它经过反复的学习，“如果让对手的棋子占据了这些地方，那么我输的概率会比较大。或者我的棋子占据了这些地方，那么我赢得概率会比较大”。1959年，出现了一个奇迹，Arthur Samuel的程序的棋艺甚至远远的超过了他自己。Arthur Samuel的西洋棋程序是第一个对于“计算机除了做程序明确让它做的事情外不能做任何事情”这种观点的有力反驳。Arthur Samuel并没有明确告诉计算机如何下棋，而是让计算机自己学习。
2、更现代化的定义
    1998年Tom Mitchell提出一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。（对于西洋棋程序的例子来说，经验E对应着程序不断和自己下棋的经历，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率）

                              二、监督学习（Supervised Learning)
之所以被称为监督学习，是因为这个算法是“监督”问题的算法，换句话说，我们给算法提供了一组“标准答案”，之后我们希望算法去学习标准输入和标准答案之间的关系以尝试对于我们的其他输入，给我们提供更为标准的答案。
监督学习有两类问题：“回归”和“分类”问题
“回归”问题（Regression）：我们需要预测的变量是连续的。
“分类”问题（Classification）：我们需要预测的变量是离散的。支持向量机的算法可以把数据映射到无限维空间中，即可以处理无限多种特性，即处理无限维的变量
Part 1：
（一）线性回归（Linear Regression)
θ被称为学习算法的参数，都是实数，利用训练集合选择或学习得到合适的参数值是学习算法的任务。
下面介绍几种算法来选取θ使得J(θ)最小：
（1）梯度下降（Gradient Descent)：从一个初始点沿着梯度下降最快的方向前进。
    梯度下降算法会重复向最陡的方向下降的步骤，
    我们可以把这个步骤写成这样的形式：
    a:= b是表示左边变量的值设成右边变量的值，表示这是计算机程序的一部分或者说是算法的一部分。mine:赋值。
    a=b是一个真值断言，想表达的是a的值与b的值相等。
    梯度下降算法中，α是一个算法参数，称为学习速度，它控制着你迈的步子有多大（你站在山上，往梯度下降最快的方向走）
    有几种不同的方式检测收敛：
    （a）检验两次迭代，看两次迭代中是否改变了很多，如果在两次迭代中没怎么改变，则可以说算法有可能收敛了。
    （b）更常用的方法是，检验的值或试图最小化的量不再发生很大的改变。

     批梯度下降算法（Batch Gradient Descent)：每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练集合样本进行求和。mine:更新一个参数需用到所有训练集
     随机梯度下降算法或增量梯度下降算法(Stochastic Gradient Descent或者incremental Gradient Descent)：每一次迭代内部会用类似于梯度下降的方式更新参数的值，最后算法结束的时候仅仅会使用i个样本。它的好处是:为了开始学习，为了开始修改参数，你仅仅需要查看第一个训练样本，你应该查看你的第一个训练样本并且利用第一个进行更新，之后你需要使用第二个训练样本执行下一次更新，这样你调整参数的速度会快的多，因为你不需要在调整之前遍历所有的样本数据。虽然随机梯度下降算法通常会快的多，但是随机梯度下降算法不会精确地收敛到全局的最小值，参数总体趋向于想着全局最小值附近徘徊，可能会一直徘徊，通常得到的参数值能够很接近全局最小值。mine：更新一个参数只需用到一个训练样本。
    如果你有一个很大的训练集合（大规模的数据集）的时候，那么你应该使用随机梯度下降算法

  (2) 正规方程组（The normal equations)

（3）局部加权回归（Locally weighted regression）线性回归的一个变化版本
    欠拟合（underfitting）：非正式的理解为一种情形，在这种情形下，数据中的某些非常明显的模式没有被成功的拟合出来。
    过拟合（overfitting）：非正式的理解为一种情形，在这种情形下，算法拟合出的结果仅仅反映了所给的特定数据的特质。例如将7个特征数据拟合出的6次多项式函数仅仅反映了该数据集合的特质，而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。
    有几种方法可以解决欠拟合及过拟合这类问题。
    （a）特征选择算法，这是一类自动化的算法，可以在这类回归问题中选择要用到的特征。
    （b）非参数学习算法，可以缓解对于选取特征的需求。
    参数学习算法（parametric learning algorithm）：是一类有固定数目的参数以用来进行数据拟合的算法。线性回归是参数学习算法的一个例子。
    非参数学习算法（non-parametric learning algorithm)：是一个参数数量会随着m（训练集合的大小）增长的算法。

    局部加权回归（Locally weighted regression）又名（loess/lowess)：
    对于你想知道x在某一位置处的y的预测值？
    线性回归的做法：首先拟合出θ使得J(θ)最小，之后返回h(x)。
    局部加权回归的做法：首先检查数据集合并且只考虑那些位于x周围固定区域内的数据点，之后对这个数据子集使用线性回归来拟合出一条直线，根据这条直线求出具体的值作为我的算法的返回结果，这将是局部加权回归中的假设的输出结果（预测值）。
    Ƭ叫做波长函数，它控制着权值随距离下降的速度，如果Ƭ越小，就会得到一个相当窄的“高斯函数”。
 （4）概率解释（Probabilistic interpretation）对于线性回归的解释
 给出一组假设来“证实”为什么我们要选择这样的指标，实际上有许多假设足够可以证实为什么我们要选择最小二乘法，这仅仅是其中之一，因为我只提供一组假设，在这组假设下最小二乘法变得有意义，但并不是唯一的一组假设，所以即使我这里描述的一组假设不成立，最小二乘法实际上在很多条件下仍有意义，这有点类似于帮助你并且给出使用最小二乘法的理由。
   ε是误差项，是对未建模的效应的捕获，它表示了一种我们没有捕获到的特征或者也可以看成一种随机噪声。
   极大似然估计，你需要选择参数θ使得数据出现的可能性尽可能的大，所以要选择θ使似然性最大化。或换句话说是选择参数使得数据数据出现的可能性尽可能的大。

Part 2 Classification and Logistic regression分类算法

（5）logistic回归（Logistic regression）一种分类算法，线性分类器
    logistic回归算法中的θ得更新公式和最小二乘回归中进行梯度下降时的公式一模一样，但是它们是完全不同的，因为logistic回归和线性回归定义不同，它已经不再是线性函数了，它是关于θ的logistic函数，所以即使它们表面上相似，相对于基于最小二乘回归退出的下降规则这实际上是一个完全不同的算法。
    实际上会以同样的学习规则结尾并不是巧合，我们之后会更多的讨论通用的学习模型，但是这是一种我们之后将会看到的最为优雅的通用的学习模型，即使我们使用不同的模型，你实际都会以一种看起来相同的学习算法结束
（6）感知器（The perceptron learning algorithm）

下面介绍另一种模型拟合的方法：牛顿算法（Newton's method）
（7）牛顿方法（Newton's method）：可以用来对logistic回归模型进行拟合，通常情况下运行速度会比梯度上升算法快很多。
和梯度上升算法比起来牛顿方法的特点：
优点：算法收敛所需要的迭代次数要少的多。
缺点：每一次迭代，都需要重新计算一次Hessian矩阵的逆，Hessian矩阵是一个n*n的矩阵（或者是n+1*n+1的矩阵，n是特征的数量），如果你要处理的问题中有大量的特征，那么Hessian矩阵的逆的运算将会花费很大的代价，但是对于规模较小，特征数量合理的问题，牛顿方法通常情况下会是一个很快的算法。

Part 3 广义线性模型（Generalized Linear Model)
线性回归算法和logistic回归算法是一类更广泛的算法的特例，这类算法被称为广义线性模型。
(8)指数分布族（The exponential family)
    选定一种函数形式，对于a(η)、b(y)、T(y),我们固定这三个函数那么这个公式就定义了一个概率分布的集合，定义了一类概率分布，这个概率分布以η为参数。

    伯努利分布和高斯分布（正态分布）都是一类分布的特例，这类分布被称为指数分布族。
其实本科中学习过的大部分分布都可以写成指数分布族的形式，正态分布、多元正态分布、伯努利分布（用于对0 1问题建模）、多项式分布（用于对有K个结果的事件建模）、泊松分布（通常用来对计数的过程进行建模，例如一个样本中放射性衰变的数目）、伽马分布、指数分布（伽马分布和指数分布考虑的是正数的分布，所以它们经常被用来对间隔进行建模）、β分布、Dirichlet分布（β分布和Dirichlet分布用来对小数进行建模，是对概率分布进行建模的）、Wishart分布（是协方差矩阵的分布）都可以写成指数分布族的形式。

(9)广义线性模型（Generalized Linear Model)——这是一组非常漂亮的概念将logistic回归和传统的最小二乘模型联系在一起。
推导出指数分布族的广义线性模型

之所以做这三个假设，是因为它会帮我们导出广义线性模型并且得到非常漂亮的算法用来拟合模型，例如泊松回归模型或者对gama分布或指数分布的输出结果进行回归。

softmax回归被普遍认为是logistic回归的推广，logistic回归只处理两种类别，softmax回归可以处理K类而不仅仅是两类。

如果你有一个机器学习问题，y属于K中的一类，你想用softmax回归来解决它，通常情况下，使用这些推导得出最后的预测，那么现在的问题就是怎样拟合出这些参数。而对于给定的任何x和θ应该用多项式分布对y进行建模，所以选择多项式分布最为指数分布族，求得最大似然估计，并且应用梯度上升或牛顿方法之类的方法使似然性最大化。

part 4 Generative Learning algorithms生成学习算法
    我们之前讨论的那些模型实质上都属于判别学习算法，
    判别学习算法：直接学习p(y|x)或学习得到一个假设直接输出0或1
    生成学习算法：用来对p(x|y)（给定所属的类的情况下，显示某种特定特征的概率，即在给定了样本所属类的情况下一个生成模型对样本特征建立概率模型）和p(y)进行建模。
    （1）高斯判别分析
    参数φ的极大似然估计的结果：训练样本中标签为1的训练样本所占的比例
    μ0 ：遍历整个训练集合，从中找到所有y=0的样本之后对于这些y=0的样本对它们做x的平均
       即选取所有的负样本并对它们的x值做平均

    当你做出了假设并使用高斯判别分析模型，其中p(x|y)服从高斯分布，当你回头计算p(y|x)时，实际上你几乎得到了和之前我们在logistic回归中使用的sigmoid函数一样的函数，但是实际上它们之间存在本质区别，高斯判别分析得到的曲线无论是位置还是陡峭程度都和logistic回归中的sigmoid函数不同。

    x|y服从高斯分布  得出   p(y=1|x)是logistic函数  反过来不一定成立
    说明x|y服从高斯分布 相对于  p(y=1|x)是logistic函数 是一个更强的假设，因为由前者可以导出后者
    所以我们可以看出高斯判别分析做出了一个更强的假设即x|y服从高斯分布，所以当这个假设正确或近似正确时，此时如果你在算法中显示地做出了这个假设，那么这个算法的表现将会很好，将会优于logistic回归，因为这个算法利用了更多关于数据的信息，算法知道数据服从高斯分布。相反地，如果你不确定x|y的分布情况，那么logistic回归的表现可能会更好。举个例子，也许你预先假设数据服从高斯分布，但是数据实际服从泊松分布，那么logistic回归仍然会获得比较好的效果，因为如果x|y服从泊松分布，p(y|x)仍然是logistic函数。但如果你用高斯判别分析的话，效果就会不太好。

    使用生成学习算法的好处与坏处
    好处：它需要更少的数据，高斯判别分析为了拟合出一个还不错的模型通常需要更少的数据即使样本数量很小，如果训练样本数量特别大或者训练样本数量特别少，生成学习算法的效果会比较好。与高斯判别分析相比，为了拟合出模型，它需要更多的样本。
    坏处：通常对数据做更强的建模假设（更准确的建模假设），如果建模假设是准确的，事实证明生成学习算法的效果通常出奇的好。相比之下，logistic回归的假设更少，对模型的假设方面更为健壮，因为你做了更弱的假设，你做了更少的假设。
    （2）朴素贝叶斯算法
    朴素贝叶斯假设：你了解到一封邮件是否为垃圾邮件以及一些词是否出现在邮件中，这些并不会帮助你预测其他的词是否出现在邮件中。
    多项式事件模型（multinomial event model)
    φ k|y=1 表示垃圾邮件中词k出现的概率
    x_i 表示邮件中第i个词的标识。

    实际上对于文本分类问题使用朴素贝叶斯算法的多项式事件模型总是比多元伯努利事件模型效果好，关于这个问题还没有被完全解释清楚，研究者们也一直在研究，关于这个原因有一个猜想是：因为它考虑了一个文档中词出现的次数。
    如果你需要处理一个文本分类问题，尽管朴素贝叶斯算法可能不是迄今为止最好的方法，但是它很容易实现，因此它是一个很不错的算法。

part 5 支持向量机————一种分类算法，非线性分类器
是一种监督学习算法，很多人认为它在众多监督学习算法中是最高效的。
实际上，朴素贝叶斯模型也属于指数分布族，所以使用朴素贝叶斯模型时得到的是logistic回归，也就是线性分类器。
神经网络————非线性分类器
为了得到了h(x)的参数,则一种为这模型学习参数的方法就是使用梯度下降法使得成本函数J(θ)(之前用过的最小二乘）最小，得到θ，从而得到h(x).
梯度下降算法应用在神经网络中时有一个专门的名字：back propagation(反向传播）
神经网络算法总是会针对于非凸优化问题，对于logistic回归，如果你运行牛顿算法或梯度下降算法，你最终会收敛得到一个全局最优值，但是对于神经网络算法来说就不是这样，通常情况下有太多的局部最优值，这使得全局优化变得更加困难，神经网络存在着严重的优化问题，你需要做很多工作它才可以工作，神经网络的工作效率还可以，就是相对复杂。
拉格朗日对偶
用对偶问题的求解代替原始问题的求解的原因：对偶问题经常会更简单，而且和原始问题相比具有更多有用的性质。

坐标上升算法与牛顿算法对比：
优点：每次迭代过程很简单，固定所有其他参数只对一个参数求最优值是比较简单的，内层循环将会执行的比较快。
缺点：坐标上升通常会经历更多的步骤

序列最小优化算法
最小指的是：一次改变最小数目的αi

                              三、学习理论（Learning Theory)
 通过学习理论，让我们理解为什么学习型算法是有效的，这样我们才可以尽可能高效的的工作。我们将对学习理论进行一些深入研究以试图了解什么样的算法能很好地近视不同的函数，并且试图了解一些诸如需要多少训练数据这样的问题。
 训练误差：给定假设h，它分类错误的训练样本在总训练集合中所占的比例。
 一般误差：根据分布D采样得到一个新的样本，对该样本分类错误的概率。
 经验风险最小化（ERM）：在假设类H中选择具有最小训练误差的假设。

 一致收敛：暗示着当m很大时，所有的^ε(h)将会同时收敛到ε(h)，此时对于所有h,其训练误差和一般误差都会非常接近。

 样本复杂度：为了达到一个特定的错误的界，你需要多大的训练集合或者多少训练样本。
 模型复杂度包括：多项式的次数、假设类H的大小

 假设类H能分散集合S：如果H可以实现该集合的任意一种标记形式。其中实现标记形式指：对于S的任意一种标记形式都可以从假设类H中找到一个假设h能够对s中的d个样本进行完美的预测。
 在二维的情况下，任意一个二维线性分类器构成的假设类都不能分散四个点构成的集合。
 对于任意维度，由n维线性分类器构成的假设类的VC维等于n+1

 样本复杂度的上界和下界都是由vc维给定的
一般情况下，vc维大概和模型的参数数量差不多，例如：n维logistic回归需要n+1个参数，而n维线性分类器的假设类的vc维是n+1，
对于多数合理的假设类，vc维总是和模型参数的数量成正比。因此直观上，训练样本的数目大致和模型的参数数量呈线性关系。
但有的情况下上述结论都不成立，会有一些奇怪的例子，模型参数很少但是vc维很大，但这些例子都不常见。
具有较大间隔的线性分类器的假设类都有比较低的vc维。
logistic回归和svm都可以看做是ERM算法的近视。








                              四、无监督学习（Unsupervised Learning)
 无监督学习：给你一组数据，然后不告诉你关于数据的任何正确答案，给你数据后会这样问你“你能在这组数据中寻找一些有趣的结构吗？”
 聚类是无监督学习的例子
                              五、强化学习（Reinforcement Learning)
强化学习：它可以被用在你不需要进行一次决策的情形中，在强化学习的过程中，你通常会在一段时间内做出一系列的决策。例如：用电脑控制飞机飞起来，你只要连续地做出一系列总体上还不错的决策，就可以让飞机飞起来。
基本概念是称为回报函数的概念