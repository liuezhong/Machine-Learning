
                             一、机器学习的定义

1、非正式定义
    1959年Arthur Samuel将机器学习非正式地定义为：在不直接针对问题进行编程的情况下，赋予计算机学习能力的研究领域。（Arthur Samuel曾经写过一个西洋棋程序，可以自己和自己下棋。Arthur Samuel让他自己的程序自己和自己下了成千上万盘棋，逐渐地，程序慢慢的意识到，怎样的局势能导致胜利，怎样的局势能导致失败。它经过反复的学习，“如果让对手的棋子占据了这些地方，那么我输的概率会比较大。或者我的棋子占据了这些地方，那么我赢得概率会比较大”。1959年，出现了一个奇迹，Arthur Samuel的程序的棋艺甚至远远的超过了他自己。Arthur Samuel的西洋棋程序是第一个对于“计算机除了做程序明确让它做的事情外不能做任何事情”这种观点的有力反驳。Arthur Samuel并没有明确告诉计算机如何下棋，而是让计算机自己学习。
2、更现代化的定义
    1998年Tom Mitchell提出一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。（对于西洋棋程序的例子来说，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率）

                              二、监督学习（Supervised Learning)
Part 1：
（一）线性回归（Linear Regression)
（1）梯度下降（Gradient Descent)：从一个初始点沿着梯度下降最快的方向前进。
    梯度下降算法会重复向最陡的方向下降的步骤，
    我们可以把这个步骤写成这样的形式：
    a:= b是表示左边变量的值设成右边变量的值，表示这是计算机程序的一部分或者说是算法的一部分。mine:赋值。
    a=b是一个真值断言，想表达的是a的值与b的值相等。
    梯度下降算法中，α是一个算法参数，称为学习速度，它控制着你迈的步子有多大（你站在山上，往梯度下降最快的方向走）
    有几种不同的方式检测收敛：
    （a）检验两次迭代，看两次迭代中是否改变了很多，如果在两次迭代中没怎么改变，则可以说算法有可能收敛了。
    （b）更常用的方法是，你检验的值或许是你试图最小化的量不再发生很大的改变。

     批梯度下降算法（Batch Gradient Descent)：每一次迭代都需要遍历整个训练集合，因为你需要基于你的m个训练集合样本进行求和。mine:更新一个参数需用到所有训练集
     随机梯度下降算法或增量梯度下降算法(Stochastic Gradient Descent或者incremental Gradient Descent)：每一次迭代内部会用类似于梯度下降的方式更新参数的值，最后算法结束的时候仅仅会使用i个样本。它的好处是:为了开始学习，为了开始修改参数，你仅仅需要查看第一个训练样本，你应该查看你的第一个训练样本并且利用第一个进行更新，之后你需要使用第二个训练样本执行下一次更新，这样你调整参数的速度会快的多，因为你不需要在调整之前遍历所有的样本数据。虽然随机梯度下降算法通常会快的多，但是随机梯度下降算法不会精确地收敛到全局的最小值，参数总体趋向于想着全局最小值附近徘徊，可能会一直徘徊，通常得到的参数值能够很接近全局最小值。mine：更新一个参数只需用到一个训练样本。
    如果你有一个很大的训练集合（大规模的数据集）的时候，那么你应该使用随机梯度下降算法

  (2)  矩阵求导

（二）局部加权回归（Locally weighted regression）线性回归的一个变化版本
    欠拟合（underfitting）：非正式的理解为一种情形，在这种情形下，数据中的某些非常明显的模式没有被成功的拟合出来。
    过拟合（overfitting）：非正式的理解为一种情形，在这种情形下，算法拟合出的结果仅仅反映了所给的特定数据的特质。例如将7个特征数据拟合出的6次多项式函数仅仅反映了该数据集合的特质，而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。
    有几种方法可以解决欠拟合及过拟合这类问题。
    （a）特征选择算法，这是一类自动化的算法，可以在这类回归问题中选择要用到的特征。
    （b）非参数学习算法，可以缓解对于选取特征的需求。
    参数学习算法（parametric learning algorithm）：是一类有固定数目的参数以用来进行数据拟合的算法。线性回归是参数学习算法的一个例子。
    非参数学习算法（non-parametric learning algorithm)：是一个参数数量会随着m（训练集合的大小）增长的算法。

    局部加权回归（Locally weighted regression）又名（loess/lowess)：
    对于你想知道x在某一位置处的y的预测值？
    线性回归的做法：首先拟合出θ使得J(θ)最小，之后返回h(x)。
    局部加权回归的做法：首先检查数据集合并且只考虑那些位于x周围固定区域内的数据点，之后对这个数据子集使用线性回归来拟合出一条直线，根据这条直线求出具体的值作为我的算法的返回结果，这将是局部加权回归中的假设的输出结果（预测值）。
    Ƭ叫做波长函数，它控制着权值随距离下降的速度，如果Ƭ越小，就会得到一个相当窄的“高斯函数”。
Part 2
（二）概率解释（Probabilistic interpretation）对于线性回归的解释
（三）logistic回归（Logistic regression）一种分类算法
（四）感知器（Digression perceptron）
（五）牛顿方法（Newton's method）一个用来对logistic回归模型进行拟合的算法


                              三、学习理论（Learning Theory)
                              四、无监督学习（Unsupervised Learning)
 聚类是无监督学习的例子
                              五、强化学习（Reinforcement Learning)
基本概念是称为回报函数的概念